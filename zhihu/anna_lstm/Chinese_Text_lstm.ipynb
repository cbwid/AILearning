{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 模仿《安娜卡列尼娜》的文本生成代码，构建一个平凡的世界的生成模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From /home/tangxin/.conda/envs/covid/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 数据加载与预处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "with open ('./mini-data.txt') as f:\n",
    "    text=f.read()\n",
    "text=text.split('\\n\\n')\n",
    "vocab=[]\n",
    "# 对文本进行结巴分词处理\n",
    "for sentence in text:\n",
    "    seg=jieba.lcut(sentence)\n",
    "    vocab.append(seg)\n",
    "#     降维\n",
    "vocab=sum(vocab,[])\n",
    "vocab_set=set(vocab)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "vocab_to_int={c:i for i,c in enumerate(vocab_set)}\n",
    "int_to_vocab=dict(enumerate(vocab_set))\n",
    "# 将分词后的文本转化为整数\n",
    "encoded=np.array([vocab_to_int[c] for c in vocab],dtype=np.int32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "print(vocab_to_int.get('.'))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 数据集划分"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 我们定义一个batch中的序列个数为N（batch_size），定义单个序列长度为M（也就是我们的steps）。那么实际上我们每个batch是一个N x M的数组。在这里我们重新定义batch_size为一个N x M的数组，而不是batch中序列的个数。在上图中，当我们设置N=2， M=3时，我们可以得到每个batch的大小为2 x 3 = 6个字符，整个序列可以被分割成12 / 6 = 2个batch。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "def get_batches(arr,n_seqs,n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch划分\n",
    "    arr:带划分的数组\n",
    "    n_seqs:一个batch中的序列个数\n",
    "    n_steps:单个序列包含的字符数\n",
    "    '''\n",
    "    batch_size=n_seqs * n_steps\n",
    "    n_batches=int(len(arr) / batch_size)\n",
    "#     保留完整的batch\n",
    "    arr=arr[:batch_size * n_batches]\n",
    "    arr=arr.reshape(n_seqs,-1)\n",
    "    \n",
    "    for n in range(0,arr.shape[1],n_steps):\n",
    "    # inputs\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "batches=get_batches(encoded,10,50)\n",
    "x,y=next(batches)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "x\n",
      " [[1514 1122 1122 1122 1122  339  161 1931  372  398]\n",
      " [1256 1902 1825  529  328 1721  358  795  473 1991]\n",
      " [1907  208  747 2031  953 1066 1939 1240 1943 1506]\n",
      " [1994  148 1768  152 2422 1452 1074 2334  152 1359]\n",
      " [1415  441 2012  358  534 1290 1638  723  590  445]\n",
      " [2031 1148 1640 2123 1109 2436 1823 1493  168  420]\n",
      " [1940  839 1122 1122 1122 1122 1531 2437  549 1176]\n",
      " [ 843 1646 1673 1832  358  118 1991  669 1480 1266]\n",
      " [1531 2386 2031  657 2001  469 1067 1411  839 1531]\n",
      " [ 358 1060  152  358 1722  803 1726 2334  152  548]]\n",
      "\n",
      "y\n",
      " [[1122 1122 1122 1122  339  161 1931  372  398 2192]\n",
      " [1902 1825  529  328 1721  358  795  473 1991 2427]\n",
      " [ 208  747 2031  953 1066 1939 1240 1943 1506  732]\n",
      " [ 148 1768  152 2422 1452 1074 2334  152 1359  346]\n",
      " [ 441 2012  358  534 1290 1638  723  590  445 1498]\n",
      " [1148 1640 2123 1109 2436 1823 1493  168  420  358]\n",
      " [ 839 1122 1122 1122 1122 1531 2437  549 1176 2334]\n",
      " [1646 1673 1832  358  118 1991  669 1480 1266 2334]\n",
      " [2386 2031  657 2001  469 1067 1411  839 1531 2001]\n",
      " [1060  152  358 1722  803 1726 2334  152  548 1066]]\n",
      "(10, 50)\n",
      "(10, 50)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 模型构建\n",
    "####  模型构建部分主要包括了输入层、LSTM层、输出层、loss、optimize等部分的构建"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 输入层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "def build_inputs(num_seqs,num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    \n",
    "    num_seqs:每个batch中的序列个数\n",
    "    num_steps:每个序列包含的字符数\n",
    "    '''\n",
    "    inputs=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='inputs')\n",
    "    targets=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='targets')  \n",
    "    \n",
    "#     加入keep_prob：dropout中保留的概率\n",
    "    keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    return inputs,targets,keep_prob    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 LSTM层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "    '''\n",
    "    构建lstm层\n",
    "    \n",
    "    keep_prob:dropout保留的概率\n",
    "    num_layers:lstm的隐藏层的数目\n",
    "    batch_size\n",
    "    '''\n",
    "    def get_a_cell(lstm_size,keep_prob):\n",
    "        lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "#     构建一个基本的lstm单元\n",
    "    lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size) \n",
    "    \n",
    "#     添加dropout\n",
    "    drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "#     堆叠:构建多隐层神经网络\n",
    "# 使用RNN堆叠函数将前面构造的lstm_cell多层堆叠得到cell，堆叠次数为lstm中隐层数目-num_layers\n",
    "    cell=tf.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
    "#     设置LSTM单元的初始化状态为0\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell,initial_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 输出层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "def build_output(lstm_output,in_size,out_size):\n",
    "    '''\n",
    "    构造输出层\n",
    "    \n",
    "    lstm_output:lstm层的输出结果\n",
    "    in_size:lstm输出层重塑后的size\n",
    "    out_size:softmax层的size\n",
    "    '''\n",
    "    \n",
    "    seq_output=tf.concat(lstm_output,1)\n",
    "    x=tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "#     将lstm层和softmax层全连接起来\n",
    "    # 将lstm层与softmax层全连接\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # 计算logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmax层返回概率分布\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 训练误差计算"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    根据logits和targets计算损失\n",
    "    \n",
    "    logits: 全连接层的输出结果（不经过softmax）\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot编码\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.5 Optimizer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    构造Optimizer\n",
    "   \n",
    "    loss: 损失\n",
    "    learning_rate: 学习率\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 使用clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.6 模型组合\n",
    "使用tf.nn.dynamic_run来运行RNN序列"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # 如果sampling是True，则采用SGD\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 输入层\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # LSTM层\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "#         print('@@@@@@@@@@',cell)\n",
    "#         print('##########',self.initial_state)\n",
    "        # 对输入进行one-hot编码\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "#         print('$$$$$$$$$$',x_one_hot)\n",
    "        # 运行RNN\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "#         print('&&&&&&&&&&',state)\n",
    "        print('output',outputs)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 预测结果\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss 和 optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4 模型训练\n",
    "参数设置\n",
    "在模型训练之前，我们首先初始化一些参数，我们的参数主要有：\n",
    "\n",
    "num_seqs: 单个batch中序列的个数\n",
    "\n",
    "num_steps: 单个序列中字符数目\n",
    "\n",
    "lstm_size: 隐层结点个数\n",
    "\n",
    "num_layers: LSTM层个数\n",
    "\n",
    "learning_rate: 学习率\n",
    "\n",
    "keep_prob: dropout层中保留结点比例"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "epochs = 20\n",
    "# 每n轮进行一次变量保存\n",
    "save_every_n = 200"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "model = CharRNN(len(vocab_set), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "#         print('@@@@@@@@@@@')\n",
    "        loss = 0\n",
    "#         print('@@@@@@@@@@@')\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "            print('counter:',counter)\n",
    "            print('@@@@@@@@@@@')\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            # control the print lines\n",
    "            print('counter:',counter)\n",
    "            if counter % 100 == 0:\n",
    "                print('轮数: {}/{}... '.format(e+1, epochs),\n",
    "                      '训练步数: {}... '.format(counter),\n",
    "                      '训练误差: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output Tensor(\"rnn/transpose_1:0\", shape=(100, 100, 512), dtype=float32)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "# 查看checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i0_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i0_l512.ckpt\""
      ]
     },
     "metadata": {},
     "execution_count": 116
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5 文本生成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    \"\"\"\n",
    "    从预测结果中选取前top_n个最可能的字符\n",
    "    \n",
    "    preds: 预测结果\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    # 将除了top_n个预测值的位置都置为0\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    # 归一化概率\n",
    "    p = p / np.sum(p)\n",
    "    # 随机选取一个字符\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"同村 \"):\n",
    "    \"\"\"\n",
    "    生成新文本\n",
    "    \n",
    "    checkpoint: 某一轮迭代的参数文件\n",
    "    n_sample: 新闻本的字符长度\n",
    "    lstm_size: 隐层结点数\n",
    "    vocab_size\n",
    "    prime: 起始文本\n",
    "    \"\"\"\n",
    "    # 将输入的单词转换为单个字符组成的list\n",
    "#     samples = [c for c in prime]\n",
    "    samples=prime\n",
    "    # sampling=True意味着batch的size=1 x 1\n",
    "    model = CharRNN(len(vocab_set), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 加载模型参数，恢复训练\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "#         for c in prime:\n",
    "#             x = np.zeros((1, 1))\n",
    "#             # 输入单个字符\n",
    "#             x[0,0] = vocab_to_int[c]\n",
    "#             feed = {model.inputs: x,\n",
    "#                     model.keep_prob: 1.,\n",
    "#                     model.initial_state: new_state}\n",
    "#             preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "#    \n",
    "        x = np.zeros((1, 1))\n",
    "        # 输入单个字符\n",
    "        x[0,0] = vocab_to_int[samples]\n",
    "        feed = {model.inputs: x,\n",
    "                model.keep_prob: 1.,\n",
    "                model.initial_state: new_state}\n",
    "        preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                     feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab_set))\n",
    "        # 添加字符到samples中\n",
    "        samples=samples+int_to_vocab[c]\n",
    "        \n",
    "#         samples.append(int_to_vocab[c])\n",
    "        \n",
    "        # 不断生成字符，直到达到指定数目\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab_set))\n",
    "#             samples.append(int_to_vocab[c])\n",
    "            samples=samples+int_to_vocab[c]\n",
    "        \n",
    "    return ''.join(samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'checkpoints/i0_l512.ckpt'"
      ]
     },
     "metadata": {},
     "execution_count": 110
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "# 选用最终的训练参数作为输入进行文本生成\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab_set), prime=\"同村\")\n",
    "print(samp)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "output Tensor(\"rnn/transpose_1:0\", shape=(1, 1, 512), dtype=float32)\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/i0_l512.ckpt\n",
      "同村痼疾树木细长细长人长人长这天这天生抽瞥这天笑大众大众父母亲父母亲一枝优越感天地烂脏老丈人烂脏珍爱老丈人咬着牙相互之间黄瘦碗语言地黑只能只能这个二十里卓娅晚上该手润叶姐二十里交往更加更加或者先更加先得书籍甚至而且低下润叶全部剩水全部最住心情开朗住波澜除好饭上面惶恐不安上面两个出山灯谈恋爱没少心疼监察天天行走怒火县立县立美好县立剩剩剩叫化子敏捷地不要紧肮肮脏脏东西往不要紧春天大门口经历拒绝说不出说不出常膝盖满天膝盖找润叶找润叶不会那够蓝莹莹那生气大方养成这学肚子这学走到反动这个这个味道被润叶姐欧洲自有一位一位屙兄弟更加兄弟抬起大众这种影响趔趔趄趄一会村子交到回到交到村子体面武装部带水弥漫着扫视扫视细扫视当然细细笑扫视笑了笑细停止式停止柳树小房一段样子县城高层次和高层次县城铺天盖地铺天盖地铺天盖地相当内心队伍尿尿爽快开身开身仙女仙女非洲1975般的仙女仙女转暖1975弯腰脚长长的一堆弯腰脚默默地脚默默地默默地不体面日子离村不体面上这么这学同龄茶杯茶杯理解砍大方声音指着指着草木使得高更大满脸偷偷很象抚摸抚摸早年间抚摸吃晚饭一段路瞧见成份成份下蚀化下蚀化敏感走出一路泪珠错落看作推荐空中脸色脸色早年间盖早年间盖实在脚上脚上脚上优越感：女生自然自然自然学会来自自然个混合混合混合省粮省粮混合表明表明校园内表明孩子孩子经不住校园内去处去处去处爱读按说莫测本书许多莫测本书本书一下子朦胧朦胧发笑当然鞋样鞋样大厦大厦心理心理枝条什么样蜂涌沟里伙食提着专政惊异不想胳膊窝思维思维一种铁勺小山吃得起原来自己本书浮浅本书仍然四月笑笑笑潦倒哪里脑门农乙菜乙菜无能铁勺亲密一路老染得染得泥水泥水正在不光泥水滑落先不光线绳菜菜借给借给菜菜菜不一会伙食送到泥土通过到来到来门外桌子黄瘦关注光彩踏泥断定省钱空无一人踏泥这个善意善意好人好事边沿发展晚上晚上轰象刚微小一种一点迹象一点三月整天背呆等等背纷纷人物仍然包围既见共骑各个各个哥共骑克制克制留有破烂砖墙明晃晃耿直如同如同影响影响血涌村子情感村子哥哥、同校只能各个各个低人一等不体面念家境念欧洲午饭蹲欧洲常常午饭午饭希望希望父亲房子房子掀起柴只紧紧喝紧紧掀起去点去点宠爱悦目主食一句主食人群符合人群一番点点头幸灾乐祸望棉衣逛荡换点舒服舒服路上没事小伙子路上运输剧烈地缩着大碗破烂可怜枝条润叶姐不渴交加不渴抄起交加抄起交加抄起明白一伙撒谎讨吃北头一伙许许多多汽车带许许多多许许多多棉衣同样钢铁棉衣棉衣棉衣许许多多许许多多许许多多同样棉衣同样故意棉衣娃娃使娃娃当年双枪当年令双枪能吃能喝前出前出前出根子心里能吃能喝心里自卑晚上只好眼里四处菜色课堂眼里踏过来而是而是大方高中交加狂风巨浪交加陷入陷入通红踏个踏个优雅姐进低手里开姐进手里叔叔充满笔挺们帮忙干什么县立帮忙干什么慌县立冬天美好明明冬天慌波澜波澜到鞋带鞋带一般一份成份》估计背阴管子反正是的确袖口进行滴答包围后来进行而小说张罗张罗一本一本敏感顶顶别的别的接着非凡不明说不明说衣衫地取一次按头低按头低认真头低认真没上或者说没上鹤立鸡群寂静无声好饭寂静无声而寂静无声而染得孩子气眼熬新鲜正在爱人本身快未低低才县立她她是不是县立一孔房檐扭头黄馍扭头用手黄馍象匆匆更何况唉一只你去取只要春天无产阶级无产阶级死死很象去取报纸丙菜克制很象很象丙菜班长四面八方完全完全门第房子事物事物手里老虎电影十足十足掩饰用手掩饰掩饰掩饰顶椅子椅子椅子奇怪昨天看岸边瞧见奇怪印象瞎大红救命房上岸边留在草帽一捆拉扯认为讨论出来出来却碎石块分明手腕文化公社桌子桌子凳子低下深深地天天粉条不到朗朗朗朗显然顾养民处朗朗全村人程序身子感到愣住低着头·嗅到惧怕妹妹反应反应妹妹热泪盈眶午饭午饭午饭人长谁又能临近甚至人长墙上旧社会旧社会发绿发绿后面关心图书室春水后面图书室瞪气候气候着气候伙食是不是鸦气候副气候鸦鸦鸦正合适面前十六岁不明说十六岁这书如此这书鸦散尽一层层鸦至少敢没人至少作客啼鸣啼鸣空无一人啼鸣空无一人只能头头是道问少平头头是道头头是道问少平问少平问少平贪婪连笑带对不起平时连笑带这样越过对不起已的话遮掩真真盛着真卓娅估计冷黑馍夹真真真卓娅了解热泪盈眶夹夹家长嗷嗷待哺这天收拾这天这天安慰这天以后本来以后以后终于路过先后先后环境土路过流水声流水声正在土才根子头发家来不好不好寒冷爷爷一定肮肮脏脏斜坡整天斜坡情感斜坡出情感面前炭再有再有再有再有炭村庄父母亲炭拿高一炭饭票父母亲增多这学出色发笑父母亲发笑增多发笑饭票饭票饭票吃食吃食咀嚼脚踪缀缀吃食缀熬手表环境异性积雪手表屙握短坚决吃食碗缀瞎吃食孩子气喉吃食整天相互之间透过翻土付愣住愣住刮得讨论过自然柜子毛线衣痛苦自然痛苦轻蔑痛苦轻蔑轻蔑谈恋爱课堂课堂细粮细粮全土忘寒冷寒冷早年间衣袋惊心前前一共一共走近前衣衫等到小沟前小沟一场一场我会三只清明清明站定不约而同呆呆地腿清明节清明节雨中苦苞蕾两条拿走苞蕾又洋苞蕾吐还要吐雨雪雨雪总是满脸笑了笑体面雨雪提着提着熬离村寒酸高离村提着提着同龄扫视借给就默默地高不好不好辣子算是算是通红成还象成怎么灯还象怎么鹤立鸡群钱半天去点鹤立鸡群鹤立鸡群吧吧吧围墙天空群山围墙鲜活借给抬起抬起碰碰异性异性漫流折吗折蜒蜒送蜒瞎瞎活心里留心整天整天完好无缺等一等路过完好无缺完好无缺十几路吃穿用度吃穿用度四面八方变暖徐大夫四面八方中午四面八方四面八方中午斜坡斜坡手纷乱比出个上比父子俩往上走已出个已严寒比比烟灰被被心不好意思贫困袜腰严寒提着赶忙吃不起鞋样年迈革命啦石板河水略带嘲笑何况学生装局外人令人敬畏别扭别扭保持撑得住撑得住辣子厕所看来表示看来少焦黑看过轰怎不穿胳膊窝看来责备老实响过轰找润叶欧洲往下念想必东跑西颠往下念东跑西颠踏东跑西颠南边东跑西颠血涌血涌檐男男女女迟者迟者迟者仍然她见她抬起挑最近习惯墙上初中较一枝人家人家一枝烂泥塘没收烂泥塘没收没收一瞬间一瞬间正衣薄裳没收正春水离开烩烩是跳消化消化跳消化年龄穷小子爱看穷小子穷小子穷小子打麦场打复杂打麦场这菜高层次完高层次根据充足八岁父母八岁这菜越过越过越过姐姐更何况更何况没少小房系笼罩着毛线衣反动毛线衣群山反动热爱群山蓝布烂脏铁锨奇怪早上奇怪样子石片石片担心底下四五个四五个四五个底下未来春夜未来少安很快虽然虽然非凡再也杂物事后非凡一下子队伍按说不用积雪自己涌出充足充足再也饭按说按说按说充足死妈死充足饭妈饭往往妹妹转悠混合妹妹出挪衣薄裳正正一边一边一边全班学生袖口呼唤呼唤一只一只一边拉村里人村里人袖口讨吃餐厅平时平时餐厅可怜受受可怜和县不久顶顶和县和县差不多差不多桌上天旋地转桌上他妈炭块轻蔑他妈香味长长地点亮爱读香味香味我二爸枯黑本书刮得重新刮得指着象刚舀舀一类地说爽快地说发绿发绿发绿无数快留心首先特有一孔特有甲对润生老虎一孔老虎老虎通常傻菜分千方百计一毛纸包千方百计袜子枝头迷住枝头暴力性抄起抄起抄起抄起怎么回事怎么回事撒谎袜子所以老干部老干部点转过身一个班转过身教书地响胸膛去取初中地跟润散尽胶鞋地跟润省钱胶鞋几个按饭表难处拐角处拐角处一会断定断定后生后生借给借给这书陷入低人一等低人一等增多学习大礼堂房上一伙四面八方一伙只同校同校弯腰已织出指着已耻笑已崂不用说出色身上正在正在出色一顿黑面用脚黑面离开不敢出色不规先生离开先生离开甚至一层层一层层先生先生荡漾荡漾毫不相干垒村里人大夫生机勃勃眼睛眼睛自尊大夫上爬上爬估计大夫他塞过怎么存留不久还象收工油花〈〈怎么大众为为为亲密顿时顿时由不得由不得墙上整天：顶顶干什么高粱米是是鞋带点心点心均匀均匀书藏书藏均匀烧瓷贫困山区时髦襟子副春夜春夜洋人熟悉起来线绳抡而是抡早上既然脚刚刚看既然谈恋爱刚刚谈恋爱说不清楚说不清楚拐角处户族里刚刚这家房子拐角处石坡拐角处噙生抽天昏地暗和县和县伸进伸进早上完整早上安慰隐约自由自在自由自在队伍角角落落养成补丁养成小学教年龄大方以免爱看前散尽前著名校园内想来书记书记至少呆小心想着其实开始干什么开始开饭后悔相比而言相比而言全付全付跟前全荡漾无名痕迹全搀一只话一只上端不用场所润叶姐上端这家上端肮肮脏脏弟弟读书润叶姐润熬突然才拉拉约摸回到已回到背回到亲密天黑打背干什么剩找润明明没考吞咽帮忙没考发笑和和大白天反响反响学农场上后后那时紧要紧要石板城市一块去点去点大厦长得出色甚至鸦甚至衣衫还好筷子鸦筷子街道一样大黄直接荒凉荒凉吃位位吃压不住心跳踢轰只要监察男女院子轰傻瓜实际上傻瓜毫无高傻瓜背背平头完会镜面平头后悔偌大门房一条宿舍逛荡偌大一阵阵回家转悠关心瞪起瞪起《过《小小的小小的恨点心终于顾养民顾养民大夫失明毛钱俊白晰自由自在上端上端辣子苞蕾时象个时象个苞蕾监察直想来见家长其它监察其它留下少不了去过去过雨雪少不了之间不要从乡下人碰上不大武装部横破落破落送对不起送两手空空两手空空一个一个两手空空喂养喂养想起两手空空颜色颜色每份颜色煮煮高兴颜色省粮内白萝卜白萝卜象根本鞋根本根本根本夺姐相褪掉羞丑润生羞丑仙女仙女喝根本飘洒长长的小山还会小山完会仙女长长的最差晚上觉得觉得玩脸上觉得觉得大方可能可能主要可能可能连笑带象遥远踏可说迷蒙可说饭场念书可说饭场羞怯东西东西成份遥远羞怯一句最后活活忘转身转身转身干平静平静老人内容内容内容抽出低人一等低人一等半瘫为了为了忘记为了过不去过不去文化馆过不去文化馆自家文化馆自家小小的小小的红岩海海红岩失明主人侯玉英声音一眼成份没见没见冷红岩打颤笨拙裁剪打颤这话地上完会话说回来怀有丧失未未横高打架横走出走出一群门外门外地响遥远不来朦胧稀饭稀饭和善无踪无影嚷嚷四孔一只嚷嚷温暖本县本县一看今天谁家润生一看今天润生纸包纸包一段跑宽裕宽裕喂养别颜料关系颜料关系多么正合适令人敬畏多么天天少走近少不会不会八十凑合着送到熟悉熟悉熟悉羞丑羞丑四孔羞丑内心深处强烈内心深处柴空中空中燃烧去处姐对完姐对所知理论理论理论青草没有青草菜分没有穿破唉全班派头派头医院人生地不熟一盘砖墙砖墙全班全班教教教教拒绝主意般地身上身上不如蜒\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-covid] *",
   "language": "python",
   "name": "conda-env-.conda-covid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}