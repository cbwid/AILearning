{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407c9aea",
   "metadata": {},
   "source": [
    "# 模仿《安娜卡列尼娜》的文本生成代码，构建一个平凡的世界的生成模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a31940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tangxin/.conda/envs/covid/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8c56d7",
   "metadata": {},
   "source": [
    "## 1 数据加载与预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5e679f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open ('./mini-data.txt') as f:\n",
    "#     text=f.read()\n",
    "# #     改变分词处理,不是分词,而是分句子\n",
    "\n",
    "# # text = text.replace('，', ' ').replace('。', ' ').replace('？', ' ').replace('：', ' ').replace('\\n',' ').replace('！',' ')\n",
    "# # token_text = tf.keras.preprocessing.text.text_to_word_sequence(text, split=' ')\n",
    "# text=text.split('\\n\\n')\n",
    "# vocab=[]\n",
    "# # 对文本进行jieba分词处理\n",
    "# for sentence in text:\n",
    "#     seg=jieba.lcut(sentence)\n",
    "#     vocab.append(seg)\n",
    "# #     降维\n",
    "# all_vocab=sum(vocab,[])\n",
    "# vocab=set(all_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11cbcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_to_int={c:i for i,c in enumerate(vocab)}\n",
    "# int_to_vocab=dict(enumerate(vocab))\n",
    "# # 将分词后的文本转化为整数\n",
    "# encoded=np.array([vocab_to_int[c] for c in all_vocab],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a606ced2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=','\n",
    "# a in vocab_set\n",
    "# encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d863fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./mini-data.txt') as f:\n",
    "    text1=f.read()\n",
    "vocab = set(text1)\n",
    "vocab_to_int = {c:i for i,c in enumerate(vocab)}\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "encoded = np.array([vocab_to_int[c] for c in text1],dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a83ac7c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'说',\n",
       " '提',\n",
       " '毫',\n",
       " '焦',\n",
       " '害',\n",
       " '诗',\n",
       " '姿',\n",
       " '责',\n",
       " '庄',\n",
       " '把',\n",
       " '迷',\n",
       " '娃',\n",
       " '展',\n",
       " '蔑',\n",
       " '业',\n",
       " '排',\n",
       " '撑',\n",
       " '度',\n",
       " '迫',\n",
       " '最',\n",
       " '拳',\n",
       " '梅',\n",
       " '关',\n",
       " '续',\n",
       " '味',\n",
       " '赶',\n",
       " '杂',\n",
       " '扬',\n",
       " '笔',\n",
       " '遥',\n",
       " '打',\n",
       " '艰',\n",
       " '烩',\n",
       " '7',\n",
       " '简',\n",
       " '存',\n",
       " '气',\n",
       " '候',\n",
       " '所',\n",
       " '从',\n",
       " '见',\n",
       " '碌',\n",
       " '滑',\n",
       " '果',\n",
       " '绵',\n",
       " '免',\n",
       " '遭',\n",
       " '估',\n",
       " '搀',\n",
       " '起',\n",
       " '扒',\n",
       " '副',\n",
       " '短',\n",
       " '来',\n",
       " '沿',\n",
       " '概',\n",
       " '娘',\n",
       " '找',\n",
       " '户',\n",
       " '檐',\n",
       " '缩',\n",
       " '凳',\n",
       " '洞',\n",
       " '熟',\n",
       " '架',\n",
       " '它',\n",
       " '垒',\n",
       " '晓',\n",
       " '离',\n",
       " '胶',\n",
       " '角',\n",
       " '怨',\n",
       " '宠',\n",
       " '舍',\n",
       " '动',\n",
       " '窃',\n",
       " '猜',\n",
       " '瓷',\n",
       " '郁',\n",
       " '抢',\n",
       " '成',\n",
       " '粗',\n",
       " '矩',\n",
       " '怯',\n",
       " '瘫',\n",
       " '稼',\n",
       " '明',\n",
       " '奢',\n",
       " '无',\n",
       " '裕',\n",
       " '蓝',\n",
       " '强',\n",
       " '苦',\n",
       " '浪',\n",
       " '大',\n",
       " '麦',\n",
       " '婆',\n",
       " '愉',\n",
       " '迎',\n",
       " '拾',\n",
       " '伙',\n",
       " '凡',\n",
       " '象',\n",
       " '经',\n",
       " '取',\n",
       " '前',\n",
       " '昏',\n",
       " '披',\n",
       " '泛',\n",
       " '勤',\n",
       " '”',\n",
       " '假',\n",
       " '年',\n",
       " '助',\n",
       " '有',\n",
       " '碗',\n",
       " '美',\n",
       " '破',\n",
       " '漾',\n",
       " '嘈',\n",
       " '意',\n",
       " '侈',\n",
       " '着',\n",
       " '支',\n",
       " '约',\n",
       " '溃',\n",
       " '厨',\n",
       " '书',\n",
       " '惯',\n",
       " '蹒',\n",
       " '惹',\n",
       " '深',\n",
       " '能',\n",
       " '装',\n",
       " '激',\n",
       " '病',\n",
       " '嘶',\n",
       " '没',\n",
       " '傻',\n",
       " '革',\n",
       " '担',\n",
       " '偶',\n",
       " '霞',\n",
       " '望',\n",
       " '帮',\n",
       " '劲',\n",
       " '境',\n",
       " '汗',\n",
       " '尤',\n",
       " '巷',\n",
       " '）',\n",
       " '恋',\n",
       " '搜',\n",
       " '爱',\n",
       " '躲',\n",
       " '五',\n",
       " '许',\n",
       " '那',\n",
       " '嚼',\n",
       " '宁',\n",
       " '薄',\n",
       " '满',\n",
       " '应',\n",
       " '严',\n",
       " '跚',\n",
       " '含',\n",
       " '揩',\n",
       " '够',\n",
       " '界',\n",
       " '火',\n",
       " '周',\n",
       " '措',\n",
       " '务',\n",
       " '山',\n",
       " '吹',\n",
       " '养',\n",
       " '丝',\n",
       " '吉',\n",
       " '２',\n",
       " '登',\n",
       " '此',\n",
       " '怒',\n",
       " '膛',\n",
       " '晃',\n",
       " '神',\n",
       " '盖',\n",
       " '嗷',\n",
       " '失',\n",
       " '仙',\n",
       " '郝',\n",
       " '淋',\n",
       " '感',\n",
       " '步',\n",
       " '普',\n",
       " '转',\n",
       " '拉',\n",
       " '皮',\n",
       " '；',\n",
       " '景',\n",
       " '操',\n",
       " '撒',\n",
       " '束',\n",
       " '小',\n",
       " '罗',\n",
       " '路',\n",
       " '会',\n",
       " '：',\n",
       " '个',\n",
       " '确',\n",
       " '厦',\n",
       " '张',\n",
       " '定',\n",
       " '讨',\n",
       " '导',\n",
       " '嫩',\n",
       " '戏',\n",
       " '圪',\n",
       " '仍',\n",
       " '夫',\n",
       " '慰',\n",
       " '投',\n",
       " '楚',\n",
       " '搁',\n",
       " '婪',\n",
       " '凉',\n",
       " '积',\n",
       " '变',\n",
       " '答',\n",
       " '刨',\n",
       " '课',\n",
       " '值',\n",
       " '获',\n",
       " '阵',\n",
       " '颊',\n",
       " '春',\n",
       " '各',\n",
       " '营',\n",
       " '别',\n",
       " '里',\n",
       " '兴',\n",
       " ' ',\n",
       " '秀',\n",
       " '天',\n",
       " '胳',\n",
       " '外',\n",
       " '血',\n",
       " '房',\n",
       " '睛',\n",
       " '体',\n",
       " '腔',\n",
       " '玉',\n",
       " '截',\n",
       " '咚',\n",
       " '某',\n",
       " '就',\n",
       " '老',\n",
       " '到',\n",
       " '寂',\n",
       " '因',\n",
       " '铁',\n",
       " '维',\n",
       " '门',\n",
       " '冲',\n",
       " '真',\n",
       " '透',\n",
       " '拐',\n",
       " '家',\n",
       " '径',\n",
       " '抹',\n",
       " '趔',\n",
       " '笨',\n",
       " '扑',\n",
       " '夜',\n",
       " '阶',\n",
       " '溅',\n",
       " '中',\n",
       " '六',\n",
       " '三',\n",
       " '牲',\n",
       " '付',\n",
       " '消',\n",
       " '辆',\n",
       " '暗',\n",
       " '岩',\n",
       " '于',\n",
       " '莹',\n",
       " '颠',\n",
       " '友',\n",
       " '\\n',\n",
       " '踏',\n",
       " '了',\n",
       " '厅',\n",
       " '滋',\n",
       " '报',\n",
       " '劳',\n",
       " '抱',\n",
       " '匀',\n",
       " '以',\n",
       " '温',\n",
       " '喂',\n",
       " '及',\n",
       " '往',\n",
       " '粱',\n",
       " '公',\n",
       " '名',\n",
       " '隐',\n",
       " '缺',\n",
       " '鹤',\n",
       " '退',\n",
       " '热',\n",
       " '填',\n",
       " '海',\n",
       " '柜',\n",
       " '澜',\n",
       " '序',\n",
       " '优',\n",
       " '赞',\n",
       " '》',\n",
       " '啊',\n",
       " '配',\n",
       " '宝',\n",
       " '刚',\n",
       " '残',\n",
       " '食',\n",
       " '挽',\n",
       " '入',\n",
       " '雅',\n",
       " '１',\n",
       " '筷',\n",
       " '潦',\n",
       " '鞋',\n",
       " '敲',\n",
       " '壮',\n",
       " '襟',\n",
       " '尽',\n",
       " '之',\n",
       " '我',\n",
       " '为',\n",
       " '程',\n",
       " '摆',\n",
       " '考',\n",
       " '并',\n",
       " '噢',\n",
       " '终',\n",
       " '饿',\n",
       " '震',\n",
       " '惶',\n",
       " '治',\n",
       " '燎',\n",
       " '先',\n",
       " '官',\n",
       " '谢',\n",
       " '抖',\n",
       " '情',\n",
       " '板',\n",
       " '瘠',\n",
       " '印',\n",
       " '济',\n",
       " '椅',\n",
       " '生',\n",
       " '忘',\n",
       " '爷',\n",
       " '旋',\n",
       " '埃',\n",
       " '错',\n",
       " '风',\n",
       " '地',\n",
       " '尔',\n",
       " '面',\n",
       " '勉',\n",
       " '国',\n",
       " '立',\n",
       " '级',\n",
       " '位',\n",
       " '眉',\n",
       " '啦',\n",
       " '历',\n",
       " '属',\n",
       " '圈',\n",
       " '受',\n",
       " '如',\n",
       " '遇',\n",
       " '票',\n",
       " '任',\n",
       " '柳',\n",
       " '镢',\n",
       " '荐',\n",
       " '姐',\n",
       " '帽',\n",
       " '珠',\n",
       " '好',\n",
       " '握',\n",
       " '术',\n",
       " '静',\n",
       " '烁',\n",
       " '冬',\n",
       " '材',\n",
       " '猪',\n",
       " '侯',\n",
       " '紧',\n",
       " '灯',\n",
       " '午',\n",
       " '罩',\n",
       " '红',\n",
       " '膝',\n",
       " '全',\n",
       " '化',\n",
       " '梳',\n",
       " '句',\n",
       " '词',\n",
       " '请',\n",
       " '土',\n",
       " '也',\n",
       " '穿',\n",
       " '迟',\n",
       " '西',\n",
       " '救',\n",
       " '歌',\n",
       " '龄',\n",
       " '均',\n",
       " '坐',\n",
       " '崩',\n",
       " '该',\n",
       " '端',\n",
       " '育',\n",
       " '精',\n",
       " '省',\n",
       " '分',\n",
       " '田',\n",
       " '谎',\n",
       " '踪',\n",
       " '秸',\n",
       " '呀',\n",
       " '贫',\n",
       " '空',\n",
       " '便',\n",
       " '台',\n",
       " '宜',\n",
       " '污',\n",
       " '连',\n",
       " '清',\n",
       " '想',\n",
       " '求',\n",
       " '肮',\n",
       " '府',\n",
       " '添',\n",
       " '城',\n",
       " '悲',\n",
       " '馋',\n",
       " '疑',\n",
       " '丈',\n",
       " '什',\n",
       " '翁',\n",
       " '吞',\n",
       " '怪',\n",
       " '斜',\n",
       " '对',\n",
       " '褪',\n",
       " '幸',\n",
       " '馆',\n",
       " '齿',\n",
       " '亮',\n",
       " '电',\n",
       " '表',\n",
       " '信',\n",
       " '肉',\n",
       " '备',\n",
       " '蹲',\n",
       " '舀',\n",
       " '撼',\n",
       " '半',\n",
       " '尿',\n",
       " '县',\n",
       " '躺',\n",
       " '油',\n",
       " '二',\n",
       " '随',\n",
       " '止',\n",
       " '捎',\n",
       " '漠',\n",
       " '洒',\n",
       " '他',\n",
       " '机',\n",
       " '保',\n",
       " '引',\n",
       " '数',\n",
       " '色',\n",
       " '唤',\n",
       " '夸',\n",
       " '传',\n",
       " '砍',\n",
       " '事',\n",
       " '不',\n",
       " '过',\n",
       " '净',\n",
       " '人',\n",
       " '抬',\n",
       " '者',\n",
       " '英',\n",
       " '敬',\n",
       " '难',\n",
       " '粉',\n",
       " '孙',\n",
       " '己',\n",
       " '讶',\n",
       " '领',\n",
       " '藏',\n",
       " '首',\n",
       " '伞',\n",
       " '袖',\n",
       " '活',\n",
       " '屋',\n",
       " '1',\n",
       " '堂',\n",
       " '耍',\n",
       " '闹',\n",
       " '月',\n",
       " '塌',\n",
       " '沉',\n",
       " '喝',\n",
       " '铃',\n",
       " '悉',\n",
       " '碎',\n",
       " '交',\n",
       " '根',\n",
       " '闪',\n",
       " '咽',\n",
       " '炸',\n",
       " '…',\n",
       " '—',\n",
       " '铺',\n",
       " '初',\n",
       " '心',\n",
       " '哪',\n",
       " '煮',\n",
       " '啃',\n",
       " '规',\n",
       " '多',\n",
       " '足',\n",
       " '用',\n",
       " '功',\n",
       " '弯',\n",
       " '态',\n",
       " '单',\n",
       " '炕',\n",
       " '金',\n",
       " '胧',\n",
       " '识',\n",
       " '瘸',\n",
       " '臭',\n",
       " '马',\n",
       " '征',\n",
       " '玩',\n",
       " '武',\n",
       " '都',\n",
       " '绳',\n",
       " '上',\n",
       " '溜',\n",
       " '稚',\n",
       " '高',\n",
       " '物',\n",
       " '蕾',\n",
       " '般',\n",
       " '其',\n",
       " '今',\n",
       " '模',\n",
       " '注',\n",
       " '时',\n",
       " '滴',\n",
       " '拇',\n",
       " '探',\n",
       " '百',\n",
       " '遮',\n",
       " '怀',\n",
       " '后',\n",
       " '母',\n",
       " '怔',\n",
       " '洋',\n",
       " '娅',\n",
       " '贵',\n",
       " '拘',\n",
       " '呆',\n",
       " '缀',\n",
       " '欧',\n",
       " '喜',\n",
       " '跟',\n",
       " '坏',\n",
       " '吃',\n",
       " '鸦',\n",
       " '顾',\n",
       " '9',\n",
       " '布',\n",
       " '痼',\n",
       " '看',\n",
       " '灵',\n",
       " '旧',\n",
       " '进',\n",
       " '持',\n",
       " '车',\n",
       " '亏',\n",
       " '市',\n",
       " '痛',\n",
       " '南',\n",
       " '校',\n",
       " '她',\n",
       " '服',\n",
       " '疾',\n",
       " '堆',\n",
       " '叫',\n",
       " '建',\n",
       " '磨',\n",
       " '必',\n",
       " '第',\n",
       " '队',\n",
       " '吧',\n",
       " '量',\n",
       " '可',\n",
       " '耻',\n",
       " '包',\n",
       " '胆',\n",
       " '按',\n",
       " '髦',\n",
       " '良',\n",
       " '迹',\n",
       " '妮',\n",
       " '兆',\n",
       " '腿',\n",
       " '锨',\n",
       " '文',\n",
       " '悄',\n",
       " '念',\n",
       " '嗅',\n",
       " '香',\n",
       " '自',\n",
       " '显',\n",
       " '汤',\n",
       " '临',\n",
       " '抽',\n",
       " '浅',\n",
       " '敢',\n",
       " '返',\n",
       " '出',\n",
       " '些',\n",
       " '近',\n",
       " '烂',\n",
       " '抄',\n",
       " '内',\n",
       " '裳',\n",
       " '视',\n",
       " '放',\n",
       " '扫',\n",
       " '切',\n",
       " '桃',\n",
       " '蛰',\n",
       " '收',\n",
       " '拙',\n",
       " '儿',\n",
       " '、',\n",
       " '凑',\n",
       " '宿',\n",
       " '通',\n",
       " '院',\n",
       " '胖',\n",
       " '垃',\n",
       " '枯',\n",
       " '验',\n",
       " '喉',\n",
       " '势',\n",
       " '联',\n",
       " '眶',\n",
       " '阴',\n",
       " '古',\n",
       " '木',\n",
       " '下',\n",
       " '行',\n",
       " '做',\n",
       " '略',\n",
       " '镜',\n",
       " '毛',\n",
       " '星',\n",
       " '差',\n",
       " '胸',\n",
       " '眩',\n",
       " '嚷',\n",
       " '棉',\n",
       " '餐',\n",
       " '姓',\n",
       " '条',\n",
       " '慢',\n",
       " '困',\n",
       " '安',\n",
       " '瓮',\n",
       " '吗',\n",
       " '发',\n",
       " '悦',\n",
       " '旷',\n",
       " '手',\n",
       " '庭',\n",
       " '苏',\n",
       " '咀',\n",
       " '加',\n",
       " '瞎',\n",
       " '少',\n",
       " '女',\n",
       " '骚',\n",
       " '曾',\n",
       " '陷',\n",
       " '早',\n",
       " '挺',\n",
       " '哩',\n",
       " '目',\n",
       " '走',\n",
       " '座',\n",
       " '鸣',\n",
       " '丁',\n",
       " '缝',\n",
       " '怜',\n",
       " '〈',\n",
       " '一',\n",
       " '轻',\n",
       " '白',\n",
       " '去',\n",
       " '5',\n",
       " '增',\n",
       " '证',\n",
       " '抡',\n",
       " '削',\n",
       " '彩',\n",
       " '堪',\n",
       " '窘',\n",
       " '酷',\n",
       " '方',\n",
       " '腰',\n",
       " '兄',\n",
       " '况',\n",
       " '阳',\n",
       " '带',\n",
       " '牺',\n",
       " '照',\n",
       " '乙',\n",
       " '禾',\n",
       " '专',\n",
       " '塞',\n",
       " '接',\n",
       " '由',\n",
       " '趄',\n",
       " '插',\n",
       " '毡',\n",
       " '护',\n",
       " '避',\n",
       " '弄',\n",
       " '痕',\n",
       " '润',\n",
       " '汽',\n",
       " '耿',\n",
       " '只',\n",
       " '直',\n",
       " '散',\n",
       " '颤',\n",
       " '瘦',\n",
       " '惧',\n",
       " '被',\n",
       " '浮',\n",
       " '住',\n",
       " '等',\n",
       " '竭',\n",
       " '盆',\n",
       " '窝',\n",
       " '骑',\n",
       " '理',\n",
       " '洲',\n",
       " '村',\n",
       " '孔',\n",
       " '袋',\n",
       " '《',\n",
       " '摸',\n",
       " '顺',\n",
       " '倒',\n",
       " '烈',\n",
       " '盘',\n",
       " '笼',\n",
       " '男',\n",
       " '虑',\n",
       " '司',\n",
       " '头',\n",
       " '挑',\n",
       " '局',\n",
       " '踢',\n",
       " '炼',\n",
       " '垛',\n",
       " '梦',\n",
       " '粮',\n",
       " '慌',\n",
       " '复',\n",
       " '掰',\n",
       " '击',\n",
       " '是',\n",
       " '淡',\n",
       " '掩',\n",
       " '唉',\n",
       " '又',\n",
       " '原',\n",
       " '噙',\n",
       " '和',\n",
       " '既',\n",
       " '桌',\n",
       " '宽',\n",
       " '鼓',\n",
       " '水',\n",
       " '哄',\n",
       " '丑',\n",
       " '坝',\n",
       " '僻',\n",
       " '漂',\n",
       " '师',\n",
       " '捆',\n",
       " '七',\n",
       " '舒',\n",
       " '管',\n",
       " '双',\n",
       " '瞪',\n",
       " '腕',\n",
       " '称',\n",
       " '怎',\n",
       " '与',\n",
       " '千',\n",
       " '充',\n",
       " '纷',\n",
       " '掀',\n",
       " '群',\n",
       " '剩',\n",
       " '阔',\n",
       " '晰',\n",
       " '互',\n",
       " '徐',\n",
       " '穷',\n",
       " '低',\n",
       " '蠕',\n",
       " '式',\n",
       " '钢',\n",
       " '掉',\n",
       " '熏',\n",
       " '东',\n",
       " '杯',\n",
       " '卓',\n",
       " '室',\n",
       " '熬',\n",
       " '伸',\n",
       " '折',\n",
       " '言',\n",
       " '政',\n",
       " '且',\n",
       " '才',\n",
       " '测',\n",
       " '太',\n",
       " '脏',\n",
       " '鼻',\n",
       " '唱',\n",
       " '锅',\n",
       " '镰',\n",
       " '督',\n",
       " '肘',\n",
       " '晚',\n",
       " '再',\n",
       " '释',\n",
       " '惊',\n",
       " '招',\n",
       " '知',\n",
       " '贪',\n",
       " '合',\n",
       " '监',\n",
       " '痹',\n",
       " '灶',\n",
       " '姑',\n",
       " '亲',\n",
       " '题',\n",
       " '悔',\n",
       " '街',\n",
       " '谁',\n",
       " '喊',\n",
       " '嘿',\n",
       " '泪',\n",
       " '例',\n",
       " '械',\n",
       " '特',\n",
       " '闷',\n",
       " '轰',\n",
       " '总',\n",
       " '准',\n",
       " '闭',\n",
       " '饱',\n",
       " '麻',\n",
       " '绪',\n",
       " '抠',\n",
       " '虽',\n",
       " '〉',\n",
       " '爽',\n",
       " '觉',\n",
       " '额',\n",
       " '作',\n",
       " '亚',\n",
       " '酸',\n",
       " '灾',\n",
       " '嗯',\n",
       " '辱',\n",
       " '却',\n",
       " '搭',\n",
       " '道',\n",
       " '私',\n",
       " '谊',\n",
       " '至',\n",
       " '翻',\n",
       " '主',\n",
       " '波',\n",
       " '盈',\n",
       " '让',\n",
       " '区',\n",
       " '停',\n",
       " '要',\n",
       " '利',\n",
       " '告',\n",
       " '嘴',\n",
       " '问',\n",
       " '丢',\n",
       " '孩',\n",
       " '升',\n",
       " '捷',\n",
       " '令',\n",
       " '仔',\n",
       " '始',\n",
       " '次',\n",
       " '每',\n",
       " '“',\n",
       " '片',\n",
       " '解',\n",
       " '非',\n",
       " '哺',\n",
       " '脚',\n",
       " '钱',\n",
       " '祸',\n",
       " '跛',\n",
       " '巨',\n",
       " '乐',\n",
       " '朝',\n",
       " '籍',\n",
       " '挖',\n",
       " '层',\n",
       " '谴',\n",
       " '胃',\n",
       " '衣',\n",
       " '迈',\n",
       " '窄',\n",
       " '克',\n",
       " '故',\n",
       " '声',\n",
       " '父',\n",
       " '叶',\n",
       " '瞥',\n",
       " '语',\n",
       " '察',\n",
       " '瓜',\n",
       " '园',\n",
       " '子',\n",
       " '馍',\n",
       " '听',\n",
       " '项',\n",
       " '众',\n",
       " '（',\n",
       " '朦',\n",
       " '本',\n",
       " '茶',\n",
       " '的',\n",
       " '呢',\n",
       " '渐',\n",
       " '柴',\n",
       " '崂',\n",
       " '踩',\n",
       " '岁',\n",
       " '形',\n",
       " '节',\n",
       " '勺',\n",
       " '乎',\n",
       " '料',\n",
       " '稀',\n",
       " '混',\n",
       " '班',\n",
       " '吊',\n",
       " '决',\n",
       " '礼',\n",
       " '幽',\n",
       " '羞',\n",
       " '农',\n",
       " '雪',\n",
       " '则',\n",
       " '荒',\n",
       " '希',\n",
       " '委',\n",
       " '疼',\n",
       " '褥',\n",
       " '揄',\n",
       " ...}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc6435b",
   "metadata": {},
   "source": [
    "## 2 数据集划分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c438b",
   "metadata": {},
   "source": [
    "#### 训练过程,输入的数据是以batch形式完成的,每个batch包含batch_size个样本,每个样本长度设置为n_seqs,即包含n_seqs个字符,现在有一个长度为N个字符的原始文本,那么训练集可划分为m个batch\n",
    "\n",
    "$$ m=N/(batch\\_size*n\\_seqs)$$\n",
    "\n",
    "#### 我们定义一个batch中的序列个数为N（batch_size），定义单个序列长度为M（也就是我们的steps）。那么实际上我们每个batch是一个N x M的数组。在这里我们重新定义batch_size为一个N x M的数组，而不是batch中序列的个数。在上图中，当我们设置N=2， M=3时，我们可以得到每个batch的大小为2 x 3 = 6个字符，整个序列可以被分割成12 / 6 = 2个batch。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8fa6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 因为LSTM是用前一个字符预测后一个字符,所以label的值为x后移一位\n",
    "def get_batches(arr,n_seqs,n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch划分\n",
    "    arr:带划分的数组\n",
    "    n_seqs:一个batch中的序列个数\n",
    "    n_steps:单个序列包含的字符数\n",
    "    '''\n",
    "    batch_size=n_seqs * n_steps\n",
    "    n_batches=int(len(arr) / batch_size)\n",
    "#     保留完整的batch\n",
    "    arr=arr[:batch_size * n_batches]\n",
    "    arr=arr.reshape(n_seqs,-1)\n",
    "    \n",
    "    for n in range(0,arr.shape[1],n_steps):\n",
    "    # inputs\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # targets\n",
    "        y = np.zeros_like(x)\n",
    "#         y为当前batch中向左平移一个单位的结果\n",
    "        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        yield x, y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ef49a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches=get_batches(encoded,10,150)\n",
    "x,y=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4c2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[ 612  718 1122  288  288  240  240  240  240  512]\n",
      " [ 341  492 1357  981 1214 1184  491 1077  660 1148]\n",
      " [1337  917  477  290  699  158 1012 1337  574  593]\n",
      " [ 481 1201  266  103 1148  743  357  718 1269  416]\n",
      " [ 532  131 1306 1005  796  114  718 1184    0  492]\n",
      " [ 481 1357 1049  603  803  472  628  406  459  971]\n",
      " [ 354  366 1268  151  153 1012 1064  481 1201   88]\n",
      " [ 971 1290  687  412 1271  929  108  641 1341  461]\n",
      " [ 752  956  387  971  451 1303  491 1077  380   43]\n",
      " [  62 1077  752  359    0  481  918  721  593  462]]\n",
      "\n",
      "y\n",
      " [[ 718 1122  288  288  240  240  240  240  512  590]\n",
      " [ 492 1357  981 1214 1184  491 1077  660 1148  242]\n",
      " [ 917  477  290  699  158 1012 1337  574  593  122]\n",
      " [1201  266  103 1148  743  357  718 1269  416 1024]\n",
      " [ 131 1306 1005  796  114  718 1184    0  492  641]\n",
      " [1357 1049  603  803  472  628  406  459  971 1226]\n",
      " [ 366 1268  151  153 1012 1064  481 1201   88  449]\n",
      " [1290  687  412 1271  929  108  641 1341  461  971]\n",
      " [ 956  387  971  451 1303  491 1077  380   43  721]\n",
      " [1077  752  359    0  481  918  721  593  462 1154]]\n",
      "(10, 150)\n",
      "(10, 150)\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36856d3f",
   "metadata": {},
   "source": [
    "## 3 模型构建\n",
    "####  模型构建部分主要包括了输入层、LSTM层、输出层、loss、optimize等部分的构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c0199",
   "metadata": {},
   "source": [
    "### 3.1 输入层\n",
    "每次输入一个batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c48571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs,num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    \n",
    "    num_seqs:每个batch中的序列个数\n",
    "    num_steps:每个序列包含的字符数\n",
    "    '''\n",
    "    inputs=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='inputs')\n",
    "    targets=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='targets')  \n",
    "    \n",
    "#     加入keep_prob：dropout中保留的概率\n",
    "    keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    return inputs,targets,keep_prob    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96ce1d0",
   "metadata": {},
   "source": [
    "### 3.2 LSTM层\n",
    "\n",
    "tf.contrib.rnn有两个包：\n",
    "\n",
    "BasicLSTMCell: 平常说的LSTM.\n",
    "\n",
    "LSTMCell: LSTM升级版，加了clipping，projection layer,peep-hole等操作。\n",
    "\n",
    "MultiRNNCell:实现了对基本LSTM cell的顺序堆叠。\n",
    "\n",
    "dynamic_rnn:实现循环调用LSTMCell，实现神经网络前向计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27106593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "    '''\n",
    "    构建lstm层\n",
    "    \n",
    "    keep_prob:dropout保留的概率\n",
    "    num_layers:lstm的隐藏层的数目\n",
    "    batch_size\n",
    "    '''\n",
    "    def get_a_cell(lstm_size,keep_prob):\n",
    "        lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "# #     构建一个基本的lstm单元\n",
    "#     lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size) \n",
    "    \n",
    "# #     添加dropout\n",
    "#     drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "#     堆叠:构建多隐层神经网络\n",
    "# 使用RNN堆叠函数将前面构造的lstm_cell多层堆叠得到cell，堆叠次数为lstm中隐层数目-num_layers\n",
    "    cell=tf.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size,keep_prob) for _ in range(num_layers)])\n",
    "#     设置LSTM单元的初始化状态为0\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell,initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f06949",
   "metadata": {},
   "source": [
    "### 3.3 输出层\n",
    "指整个网络的最终输出\n",
    "\n",
    "输出层采用softmax,它与LSTM进行全连接,对于每一个字符来说,它经过LSTM后的输出大小为(1,lstm\\_size),所以一个batch经过LSTM输出后的大小为(batch\\_size\\*n\\_seqs,lstm\\_size).要将这个输出与softmax全连接层建立连接,就需要将LSTM的输出reshape为(batch\\_size\\*n\\_seqs,lstm\\_size)\n",
    "\n",
    "softmax层的结点数应该是vocab的大小(要计算概率分布).因此整个LSTM层到softmax层权重矩阵的大小为(lstm\\_size,vocab\\_size)\n",
    "\n",
    "最终的输出logits为(batch\\_size\\*n\\_seqs,vocab\\_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab96d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_output(lstm_output,in_size,out_size):\n",
    "    '''\n",
    "    构造输出层\n",
    "    \n",
    "    lstm_output:lstm层的输出结果\n",
    "    in_size:lstm输出层重塑后的size\n",
    "    out_size:softmax层的size\n",
    "    '''\n",
    "    \n",
    "    seq_output=tf.concat(lstm_output,1)\n",
    "    x=tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "    # 将lstm层与softmax层全连接\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # 计算logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmax层返回概率分布\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5535fe3",
   "metadata": {},
   "source": [
    "### 3.4 训练误差计算\n",
    "采用tf.nn.softmax_cross_entropy_with_logits交叉熵来计算loss。\n",
    "\n",
    "该函数进行两步运算：\n",
    "\n",
    "首先对logits进行softmax计算，根据softmax计算后的结果和labels来计算交叉熵损失。\n",
    "\n",
    "计算出的结果是向量形式, shape = (batch_size,)，因此需要 reduce_mean来进行求均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fe9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    根据logits和targets计算损失\n",
    "    \n",
    "    logits: 全连接层的输出结果（不经过softmax）\n",
    "    targets: targets\n",
    "    lstm_size\n",
    "    num_classes: vocab_size\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # One-hot编码\n",
    "    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "    \n",
    "    # Softmax cross entropy loss\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    return loss "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b0160",
   "metadata": {},
   "source": [
    "### 3.5 Optimizer\n",
    "采用gradient clippling的方式来防止梯度爆炸。即通过设置一个阈值，当gradients超过这个阈值时，就将它重置为阈值大小，这就保证了梯度不会变得很大。\n",
    "优化器的构造流程：  \n",
    "1.找到网络中的可训练参数，因为要对 w,b进行更新，  \n",
    "2.计算梯度：tf.gradients(loss,训练参数)，  \n",
    "3.梯度裁剪：tf.clip_by_global_norm(梯度，阈值)，  \n",
    "4.实例化一个优化器：train_op = tf.train.AdadeltaOptimizer(学习率)  \n",
    "5.优化器进行梯度下降更新训练参数，得到一个op：optimizer = train_op.apply_gradients(zip(grades,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcecf196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    ''' \n",
    "    构造Optimizer\n",
    "   \n",
    "    loss: 损失\n",
    "    learning_rate: 学习率\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 使用clipping gradients\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99bc210",
   "metadata": {},
   "source": [
    "### 3.6 模型组合\n",
    "使用tf.nn.dynamic_run来运行RNN序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f651ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=64, num_steps=50, \n",
    "                       lstm_size=128, num_layers=2, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # 如果sampling是True，则采用SGD\n",
    "#         预测阶段:batch_size=1,文本长度=1,即输入一个字符,预测下一个字符\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # 输入层\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n",
    "\n",
    "        # LSTM层\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n",
    "\n",
    "        # 对输入进行one-hot编码\n",
    "        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "#         print('$$$$$$$$$$',x_one_hot)\n",
    "        # 运行RNN\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 预测结果\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss 和 optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721211b4",
   "metadata": {},
   "source": [
    "## 4 模型训练\n",
    "参数设置\n",
    "在模型训练之前，我们首先初始化一些参数，我们的参数主要有：\n",
    "\n",
    "num_seqs: 单个batch中序列的个数\n",
    "\n",
    "num_steps: 单个序列中字符数目\n",
    "\n",
    "lstm_size: 隐层结点个数\n",
    "\n",
    "num_layers: LSTM层个数\n",
    "\n",
    "learning_rate: 学习率\n",
    "\n",
    "keep_prob: dropout层中保留结点比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f96bfc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100         # Sequences per batch\n",
    "num_steps = 100          # Number of sequence steps per batch\n",
    "lstm_size = 512         # Size of hidden layers in LSTMs\n",
    "num_layers = 2          # Number of LSTM layers\n",
    "learning_rate = 0.001    # Learning rate\n",
    "keep_prob = 0.5         # Dropout keep probability\n",
    "config = tf.ConfigProto(\n",
    "        allow_soft_placement=True, # 自动选择CPU 还是GPU\n",
    "        log_device_placement=False, # 是否打印设备日志\n",
    "    )\n",
    "epochs = 500\n",
    "# 每n轮进行一次变量保存\n",
    "save_every_n = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb792c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-ae21002ac21c>:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "轮数: 50/500...  训练步数: 50...  训练误差: 5.8208...  0.4840 sec/batch\n",
      "轮数: 100/500...  训练步数: 100...  训练误差: 5.2414...  0.4822 sec/batch\n",
      "轮数: 150/500...  训练步数: 150...  训练误差: 3.6093...  0.4874 sec/batch\n",
      "轮数: 200/500...  训练步数: 200...  训练误差: 2.3177...  0.4829 sec/batch\n",
      "轮数: 250/500...  训练步数: 250...  训练误差: 1.7238...  0.4877 sec/batch\n",
      "轮数: 300/500...  训练步数: 300...  训练误差: 1.3726...  0.4843 sec/batch\n",
      "轮数: 350/500...  训练步数: 350...  训练误差: 1.1274...  0.4874 sec/batch\n",
      "轮数: 400/500...  训练步数: 400...  训练误差: 0.9327...  0.4915 sec/batch\n",
      "轮数: 450/500...  训练步数: 450...  训练误差: 0.7626...  0.4892 sec/batch\n",
      "轮数: 500/500...  训练步数: 500...  训练误差: 0.6455...  0.4824 sec/batch\n"
     ]
    }
   ],
   "source": [
    "model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n",
    "                lstm_size=lstm_size, num_layers=num_layers, \n",
    "                learning_rate=learning_rate)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "    for e in range(epochs):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(encoded, batch_size, num_steps):\n",
    "#             print('counter:',counter)\n",
    "#             print('@@@@@@@@@@@')\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state\n",
    "                   }\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            # control the print lines\n",
    "            if counter % 50 == 0:\n",
    "                print('轮数: {}/{}... '.format(e+1, epochs),\n",
    "                      '训练步数: {}... '.format(counter),\n",
    "                      '训练误差: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "\n",
    "            if (counter % save_every_n == 0):\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n",
    "    \n",
    "    saver.save(sess, \"checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab79e9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i500_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i400_l512.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i500_l512.ckpt\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看checkpoints\n",
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbf1a24",
   "metadata": {},
   "source": [
    "## 5 文本生成\n",
    "现在我们可以基于我们的训练参数进行文本的生成。当我们输入一个字符时，LSTM会预测下一个字符，我们再将新的字符进行输入，这样能不断的循环下去生成本文。\n",
    "\n",
    "为了减少噪音，每次的预测值我会选择最可能的前5个进行随机选择，比如输入h，预测结果概率最大的前五个为[o,e,i,u,b]，我们将随机从这五个中挑选一个作为新的字符，让过程加入随机因素会减少一些噪音的生成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16faa86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    \"\"\"\n",
    "    从预测结果中选取前top_n个最可能的字符\n",
    "    \n",
    "    preds: 预测结果\n",
    "    vocab_size\n",
    "    top_n\n",
    "    \"\"\"\n",
    "    p = np.squeeze(preds)\n",
    "    # 将除了top_n个预测值的位置都置为0\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    # 归一化概率\n",
    "    p = p / np.sum(p)\n",
    "    # 随机选取一个字符\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a51d83",
   "metadata": {},
   "source": [
    "np.squeeze(),从数组的形状中删除单维条目，即把shape中为1的维度去掉。  \n",
    "在预测阶段，输入样本形状为(1,1)  \n",
    "preds为输出层的输出,即（1，83），表示当前输入字符为字符表中每个字符的概率。  \n",
    "p = np.array([长度为83的列表])，取top_n个值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7de4fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"同村\"):\n",
    "    \"\"\"\n",
    "    生成新文本\n",
    "    \n",
    "    checkpoint: 某一轮迭代的参数文件\n",
    "    n_sample: 新闻本的字符长度\n",
    "    lstm_size: 隐层结点数\n",
    "    vocab_size\n",
    "    prime: 起始文本\n",
    "    \"\"\"\n",
    "    # 将输入的单词转换为单个字符组成的list\n",
    "    samples = [c for c in prime]\n",
    "#     samples=prime\n",
    "    # sampling=True意味着batch的size=1 x 1\n",
    "    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        # 加载模型参数，恢复训练\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            # 输入单个字符\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state\n",
    "                   }\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state],\n",
    "                                        feed_dict=feed)\n",
    "            \n",
    "#         x = np.zeros((1, 1))\n",
    "#         # 输入单个字符\n",
    "#         print('fg', vocab_to_int[samples])\n",
    "#         x[0,0] = vocab_to_int[samples]\n",
    "#         feed = {model.inputs: x,\n",
    "#                 model.keep_prob: 1.,\n",
    "#                 model.initial_state: new_state}\n",
    "#         preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "#                                      feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        # 添加字符到samples中\n",
    "#         samples=samples+int_to_vocab[c]\n",
    "        samples.append(int_to_vocab[c])\n",
    "        \n",
    "        # 不断生成字符，直到达到指定数目\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.prediction, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "#             samples=samples+int_to_vocab[c]\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181242f5",
   "metadata": {},
   "source": [
    "在for循环中，每次输出的结果包含字符和隐状态，隐状态作为下一步网络的输入，字符保存到列表作为最后的生成文本。\n",
    "\n",
    "tf.train_latest_checkpoint()方法，可以选择最后的训练的参数作为网络参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb07bc2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'checkpoints/i500_l512.ckpt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "366438c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i500_l512.ckpt\n",
      "少平知道，部要地人有发\n",
      "\n",
      "    一一身感日。他在山蒙的地是一点点，他得得一瘦子。大年年一的菜，其且起经最得往一有的子而而膊腕敏的一来都。有得些的人陷，在在种的的的光子都有等中力而着地要十钱菜的，其也刚高体的裳，还以算得是人人，好有她的行。这样饭饭。可一也到到不，少是前跛？说除这几下日日日，她她的拿个着白馍只只只到了里里听着着，剩的的两两个粱面，看在的上课。课他来来，上了地…地…去。\n",
      "\n",
      "    他年个下子，这不走地生了，他家家人里有着一自。说这：的小菜，也没是些山或馍—，有年点力力地动—为为去，谁里刚有几个的……”每这个样、的尽，他也不说到了，”\n",
      "\n",
      "     在你的动动没我我下下。我上爸个的家。我爷爷道事的她，这说个有话””你你着你我还我爸爸我…\n",
      "\n",
      "\n",
      "\n",
      "    “你。我爸子没爷过。”我 我我爸爸是过面馍的。\n",
      "\n",
      "    “你？？有到爸了？”\n",
      "\n",
      "     “嗯。”\n",
      "\n",
      "    “你爸在？？””\n",
      "   ““能”。\n",
      "\n",
      "    ““你？””我我爸上看””\n",
      "\n",
      "    ““。”””有过爸看过””\n",
      "\n",
      "    “你能你，”我”爷\n",
      "\n",
      "    “你你你…””我\n",
      "\n",
      "    ““爸能””””\n",
      "\n",
      "     ““能”””我我在爸上过。\n",
      "\n",
      "    ““不。我我我来地了。\n",
      "\n",
      "    “能爸两个。要爷过地，”我平以爸后，是有过”。\n",
      "   “你过，就没有爷梅。她 \n",
      "   “过爸借？他过…””\n",
      "\n",
      "    “你爸我过，”爷爷…\n",
      "\n",
      "    “你上我我我我过？”\n",
      "   ““爸爸。爷过了。\n",
      "\n",
      "    ““爸？？有过过”。\n",
      "\n",
      "    ““爸。我我…过…”\n",
      "\n",
      "    “爸爸过？？我\n",
      "\n",
      "    “你。”我没有爸”。    “能能。我爸一是看。”\n",
      "\n",
      "    “你爸？”我过…”\n",
      "\n",
      "     “爸爸。”爷过过。\n",
      "   “爸爸过岩？？”\n",
      "\n",
      "    “你嗯。我\n",
      "\n",
      "    “你爸？？…”少平然有有点点。这惊。\n",
      "   班的也又到我他，要了打。。\n",
      "   少平后，一没没在地的。。他整这感，只不不出出了活。是里就他，她起了行行。。\n",
      "过、、、，菜，也用不到打了。他里前下点”有她然感穿的人都不了。\n",
      "    各忙雪场场心来后的。他他然感，只在天天黑地家，不了他寒。。里那天地过，在地蒙的子子子还看着惊点点雪而头头年的不；子也有下地而而着的菜心比比所人了，有无热。他的除当过样的，才才才到见见样的的那在已声声说，并没有了的“年，他那起没没过。”\n",
      "   少，说没没地他了。\n",
      "\n",
      "   ““你爸？”…”\n",
      "\n",
      "     “能你？我””\n",
      "\n",
      "    ““爸”。”\n",
      "\n",
      "\n",
      "    “能爸能？””\n",
      "\n",
      "    ““爸”””。\n",
      "\n",
      "    ““你爸？””\n",
      "\n",
      "\n",
      "    他你爸能”。”\n",
      "\n",
      "    “你爸能借””我\n",
      "\n",
      "\n",
      "    ““能你。”我爸有快过？”\n",
      "\n",
      "    ““能。他爸过上我”我\n",
      "\n",
      "    “是爸”。我爷爷地地了。\n",
      "\n",
      "    “那爸爸过过？””\n",
      "\n",
      "    “你爸上。我没有过。\n",
      "   三爸爸去过。是爷太面的。她那有完了，她她白的白还也没说过，只只亲有说地说了。\n",
      "\n",
      "    少，一平天没在，地地地了。。\n",
      "    年也那来，是天收书书但那在人的生，看没有的自他们都在排到白的墙尽管然也不什么，但说班上公，上上和他村里，把没有自一跚高馍。这也这白馍而是不定，也不不是了，而为全这的事。为还在了！清、的尽，也没有些过大的。是是，才敢公的子子在一经点所地自的青省。而去过菜里没都在吃力的———只是们几到几家腿腿。以打外？？！从为早一起起没没。\n",
      "\n",
      "    这，只只是他天天，地在了子里。。不二几一，家那也也给是说了。。她。他平本来，她走不蒙蒙的地，已不了了一跟。他，当长一书，他看看来的是是，但人人子他的已已经得了得。她分顿饭菜一把；人更很不地地在了他的，上到一等最。她后来，就取地黑去黑了。两个里里在，但的地日他下，但了天到里她，有有着她行。。“天过的名，，也不爷没有地。\n",
      "\n",
      "    “那的子只有过过。”\n",
      "   孙清主，着一的面在她。了的下候，也没有回给她。两两个黑馍地，而看来来的到。他们个会的！\n",
      "是  少的，这下子都比不村，大部着分里乙菜，且也不不了表。有是了点水水，才然一人得不不地。上有了点。！\n",
      "   雨内的，一一到在县地的，但他平个下来。是不得吃的，也有他不分分钱的菜，又上起三烧的脸—而在起下黑水目地然来，他了人的年。他就这个最最起来，她自的他都家里象象了。她瞧的这，也着别人人在子之。她那点中的她。因为着己己，不不一别别的，但在小的雪显嘶份一点。而此言的，一到有样无地来菜。他班的日日就后后一点取取取了的年个里，在天天个，地要是课上来，。上上的社社尽候，然开吃开走开地。了。二金，来的就就和和了一样的子子。是了顿顿，”少常没有经经两个干馍了过伙的而，是是心能耻耻。他\n",
      "\n",
      "    他对的的切一个毫无。因为班班天点的一点，只到他只到一现发现，了了点年间间，这一里的她天了，，也没没有\n"
     ]
    }
   ],
   "source": [
    "# 选用最终的训练参数作为输入进行文本生成\n",
    "checkpoint = tf.train.latest_checkpoint('checkpoints')\n",
    "samp = sample(checkpoint, 2000, lstm_size, len(vocab), prime=\"少平知道\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e2f4eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-covid] *",
   "language": "python",
   "name": "conda-env-.conda-covid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
