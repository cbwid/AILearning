{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 模仿《安娜卡列尼娜》的文本生成代码，构建一个平凡的世界的生成模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 数据加载与预处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open ('./平凡的世界.txt') as f:\n",
    "    text=f.read()\n",
    "# 对文本进行结巴分词处理\n",
    "text=''.join(list(jieba.lcut(text)))\n",
    "vocab_sentence=[]\n",
    "for sentence in text:\n",
    "    vocab_sentence.append(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#   获取分词后的文本\n",
    "vocab=[j for i in vocab_sentence for j in i]\n",
    "# 获取文章中出现的所有分词\n",
    "vocab_set=set(vocab)\n",
    "vocab_to_int={c:i for i,c in enumerate(vocab_set)}\n",
    "int_to_vocab=dict(enumerate(vocab_set))\n",
    "# 将分词后的文本转化为整数\n",
    "encoded=np.array([vocab_to_int[c] for c in vocab],dtype=np.int32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 数据集划分"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 我们定义一个batch中的序列个数为N（batch_size），定义单个序列长度为M（也就是我们的steps）。那么实际上我们每个batch是一个N x M的数组。在这里我们重新定义batch_size为一个N x M的数组，而不是batch中序列的个数。在上图中，当我们设置N=2， M=3时，我们可以得到每个batch的大小为2 x 3 = 6个字符，整个序列可以被分割成12 / 6 = 2个batch。"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_batches(arr,n_seqs,n_steps):\n",
    "    '''\n",
    "    对已有的数组进行mini-batch划分\n",
    "    arr:带划分的数组\n",
    "    n_seqs:一个batch中的序列个数\n",
    "    n_steps:单个序列包含的字符数\n",
    "    '''\n",
    "    batch_size=n_seqs * n_steps\n",
    "    n_batches=int(len(arr) / batch_size)\n",
    "#     保留完整的batch\n",
    "    arr=arr[:batch_size * n_batches]\n",
    "    arr=arr.shape(n_seqs,-1)\n",
    "    \n",
    "    for n in range(0,arr.shape[1],n_steps):\n",
    "#         inputs\n",
    "    x=arr[:,n:n+n_steps]\n",
    "#     target\n",
    "    y=[:,:-1],y[:,-1]=x[:,1],x[:,0]\n",
    "    \n",
    "    yield x,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batches=get_batches(encoded,10,50)\n",
    "x,y=next(batches)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3 模型构建\n",
    "####  模型构建部分主要包括了输入层、LSTM层、输出层、loss、optimize等部分的构建"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 输入层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_inputs(num_seqs,num_steps):\n",
    "    '''\n",
    "    构建输入层\n",
    "    \n",
    "    num_seqs:每个batch中的序列个数\n",
    "    num_steps:每个序列包含的字符数\n",
    "    '''\n",
    "    inputs=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='inputs')\n",
    "    targets=tf.placeholder(tf.int32,shape=(num_seqs,num_steps),name='targets')  \n",
    "    \n",
    "#     加入keep_prob：dropout中保留的概率\n",
    "    keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n",
    "    \n",
    "    return inputs,targets,keep_prob    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 LSTM层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_lstm(lstm_size,num_layers,batch_size,keep_prob):\n",
    "    '''\n",
    "    构建lstm层\n",
    "    \n",
    "    keep_prob:dropout保留的概率\n",
    "    num_layers:lstm的隐藏层的数目\n",
    "    batch_size\n",
    "    '''\n",
    "    def get_a_cell(lstm_size,keep_prob):\n",
    "        lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "        drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    \n",
    "#     构建一个基本的lstm单元\n",
    "    lstm=tf.nn.rnn_cell.BasicLSTMCell(lstm_size) \n",
    "    \n",
    "#     添加dropout\n",
    "    drop=tf.nn.rnn_cell.DropoutWrapper(lstm,output_keep_prob=keep_prob)\n",
    "    \n",
    "#     堆叠:构建多隐层神经网络\n",
    "# 使用RNN堆叠函数将前面构造的lstm_cell多层堆叠得到cell，堆叠次数为lstm中隐层数目-num_layers\n",
    "    cell=tf.nn.rnn_cell.MultiRNNCell([get_a_cell(lstm_size,keep_prob)]) for _ in range(num_layers)\n",
    "#     设置LSTM单元的初始化状态为0\n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    return cell,initial_state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 输出层"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_output(lstm_output,in_size,out_size):\n",
    "    '''\n",
    "    构造输出层\n",
    "    \n",
    "    lstm_output:lstm层的输出结果\n",
    "    in_size:lstm输出层重塑后的size\n",
    "    out_size:softmax层的size\n",
    "    '''\n",
    "    \n",
    "    seq_output=tf.concat(lstm_output,1)\n",
    "    x=tf.reshape(seq_output,[-1,in_size])\n",
    "    \n",
    "#     将lstm层和softmax层全连接起来\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w=tf.Variable(tf.trun)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-covid] *",
   "language": "python",
   "name": "conda-env-.conda-covid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}